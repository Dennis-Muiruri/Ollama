{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f704696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " ** LLM Response (Streaming) :** \n",
       "\n",
       "I couldn't find any information on a well-known insurance company called \"Jubille Insurance\" in Kenya or elsewhere. It's possible that the company is small, local, or relatively new.\n",
       "\n",
       "However, I can suggest some alternatives to help you find more information:\n",
       "\n",
       "1. **Google search**: Try searching for \"Jubille Insurance Kenya\" on Google to see if any results come up.\n",
       "2. **Kenya's insurance regulator website**: Visit the website of the Insurance Regulatory Authority (IRA) in Kenya, which oversees and regulates all insurance companies operating in the country.\n",
       "3. **Local business directories**: Look for local business directories or listings that may have information about Jubille Insurance.\n",
       "\n",
       "If you're interested, I can also suggest some well-known insurance companies operating in Kenya:\n",
       "\n",
       "1. **Kenya Reinsurance Corporation**\n",
       "2. **Resolution Insurance Brokers Ltd**\n",
       "3. **UAP Holdings Plc** (one of the largest insurance groups in East Africa)\n",
       "4. **Sanlam Kenya**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "# Directly simulate streaming with ollama.chat\n",
    "prompt = \"Tell me about Jubille Insurance? it's in kenya\"\n",
    "\n",
    "# Call ollama.chat with streaming enabled\n",
    "response_stream = ollama.chat(\n",
    "model='llama3.1:8b',\n",
    "messages=[{'role': 'user', 'content': prompt}],\n",
    "stream=True # Enable streaming\n",
    ")\n",
    "    \n",
    "# Initialize an empty response string\n",
    "streamed_response = \"\"\n",
    "\n",
    "# Iterate through the streamed tokens\n",
    "for token in response_stream:\n",
    "# Append the current token to the response\n",
    "     streamed_response += token['message']['content']\n",
    "\n",
    "# Clear previous output and display the updated response\n",
    "     clear_output(wait=True)\n",
    "     display(Markdown(f\" ** LLM Response (Streaming) :** \\n\\n{streamed_response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be0ff976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2-vision', 'created_at': '2024-11-14T15:00:13.5912452Z', 'message': {'role': 'assistant', 'content': \"The image depicts a man standing on the curb of a street, with a row of cars parked along the side. The man is wearing a dark-colored shirt and blue jeans, and has his hands in his pockets. He appears to be waiting for something or someone.\\n\\nIn the background, there are several trees and buildings visible, suggesting that the image was taken in an urban setting. The overall atmosphere of the image is one of casualness and relaxation, as the man seems to be enjoying the view and taking his time.\\n\\nIt's worth noting that the image may have been edited or filtered in some way, as the colors and lighting appear to be slightly enhanced. Additionally, there may be other elements present in the original image that are not visible in this version.\"}, 'done_reason': 'stop', 'done': True, 'total_duration': 633133612500, 'load_duration': 147566900, 'prompt_eval_count': 18, 'prompt_eval_duration': 548323000000, 'eval_count': 153, 'eval_duration': 83314000000}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': 'What is in this image?',\n",
    "        'images': ['12.jpg']\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b93067",
   "metadata": {},
   "source": [
    "### understanding code better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama run qwen2.5-coder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf1ca2",
   "metadata": {},
   "source": [
    "### using ollama 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90b68922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " ** LLM Response (Streaming) :** \n",
       "\n",
       "The image depicts a man standing on a sidewalk, with a row of cars parked behind him. The purpose of the image is to showcase the man's outfit and accessories.\n",
       "\n",
       "* A man:\n",
       "\t+ Standing on a sidewalk\n",
       "\t+ Wearing a black t-shirt with a white design\n",
       "\t+ Wearing blue jeans\n",
       "\t+ Wearing orange boots\n",
       "\t+ Has short black hair\n",
       "\t+ Looking at the camera with a neutral expression\n",
       "* Cars:\n",
       "\t+ Parked behind the man\n",
       "\t+ Various makes and models\n",
       "\t+ Some have their headlights on, while others do not\n",
       "\t+ One car has a license plate that reads \"KJA 123\"\n",
       "* A sidewalk:\n",
       "\t+ Made of stone or brick\n",
       "\t+ Has a yellow curb running along its edge\n",
       "\t+ Is lined with greenery, including trees and bushes\n",
       "\n",
       "The image suggests that the man is posing for a photo shoot, possibly to showcase his fashion sense. The presence of cars in the background adds context to the scene, but does not appear to be directly related to the man's outfit or accessories."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "# Directly simulate streaming with ollama.chat\n",
    "prompt = \"tell me about this picture?\"\n",
    "\n",
    "# Call ollama.chat with streaming enabled\n",
    "response_stream = ollama.chat(\n",
    "model='llama3.2-vision',\n",
    "messages=[{'role': 'user', 'content': prompt,'images': ['12.jpg']}],\n",
    "stream=True # Enable streaming\n",
    ")\n",
    "    \n",
    "# Initialize an empty response string\n",
    "streamed_response = \"\"\n",
    "\n",
    "# Iterate through the streamed tokens\n",
    "for token in response_stream:\n",
    "# Append the current token to the response\n",
    "     streamed_response += token['message']['content']\n",
    "\n",
    "# Clear previous output and display the updated response\n",
    "     clear_output(wait=True)\n",
    "     display(Markdown(f\" ** LLM Response (Streaming) :** \\n\\n{streamed_response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33563dc",
   "metadata": {},
   "source": [
    "### using llama in jubilee insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1537f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CAPTUREDATE TREATMENTDATE RECEIVEDDATE DISCHARGEDATE INVOICEDATE  \\\n",
      "0  2024-01-01    2024-01-02   2024-01-04    2024-01-03  2024-01-06   \n",
      "1  2024-01-05    2024-01-08   2024-01-07    2024-01-09  2024-01-11   \n",
      "\n",
      "  INVOICELINEUSERSTATUS POLICYSTATUS AUTHENTICATION_TYPE FIRSTDIAGNOSIS  \\\n",
      "0                  Paid         Live          AUTHORISED    Diagnosis A   \n",
      "1   NP - Not to be Paid      Expired        UNAUTHORISED    Diagnosis B   \n",
      "\n",
      "      Prescription  Prescribed Amount  INVOICEDAMOUNT  Days_Treatment  \\\n",
      "0  Prescription A1                500             500               1   \n",
      "1  Prescription B1                400             400               3   \n",
      "\n",
      "   Days_Received  Days_Discharge  Days_Invoice   Status  \n",
      "0              3               2             5  proceed  \n",
      "1              2               4             6  proceed  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import ollama  # Assuming LLaMA's API client is available as `ollama`\n",
    "\n",
    "# Sample data for demonstration purposes\n",
    "data = {\n",
    "    'CAPTUREDATE': ['2024-01-01', '2024-01-05', '2024-01-10'],\n",
    "    'TREATMENTDATE': ['2024-01-02', '2024-01-08', '2024-01-15'],\n",
    "    'RECEIVEDDATE': ['2024-01-04', '2024-01-07', '2024-01-16'],\n",
    "    'DISCHARGEDATE': ['2024-01-03', '2024-01-09', '2024-01-18'],\n",
    "    'INVOICEDATE': ['2024-01-06', '2024-01-11', '2024-01-20'],\n",
    "    'INVOICELINEUSERSTATUS': ['Paid', 'NP - Not to be Paid', 'Paid'],\n",
    "    'POLICYSTATUS': ['Live', 'Expired', 'Live'],\n",
    "    'AUTHENTICATION_TYPE': ['AUTHORISED', 'UNAUTHORISED', 'Blank'],\n",
    "    'FIRSTDIAGNOSIS': ['Diagnosis A', 'Diagnosis B', 'Diagnosis A'],\n",
    "    'Prescription': ['Prescription A1', 'Prescription B1', 'Prescription A2'],\n",
    "    'Prescribed Amount': [500, 400, 300],\n",
    "    'INVOICEDAMOUNT': [500, 400, 250]  # One mismatch for testing\n",
    "}\n",
    "\n",
    "df_new = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Convert date columns to datetime format\n",
    "for col in ['CAPTUREDATE', 'TREATMENTDATE', 'RECEIVEDDATE', 'DISCHARGEDATE', 'INVOICEDATE']:\n",
    "    df_new[col] = pd.to_datetime(df_new[col])\n",
    "# Step 2: Calculate day differences\n",
    "df_new['Days_Treatment'] = (df_new['TREATMENTDATE'] - df_new['CAPTUREDATE']).dt.days\n",
    "df_new['Days_Received'] = (df_new['RECEIVEDDATE'] - df_new['CAPTUREDATE']).dt.days\n",
    "df_new['Days_Discharge'] = (df_new['DISCHARGEDATE'] - df_new['CAPTUREDATE']).dt.days\n",
    "df_new['Days_Invoice'] = (df_new['INVOICEDATE'] - df_new['CAPTUREDATE']).dt.days\n",
    "\n",
    "# Step 3: Filter based on INVOICELINEUSERSTATUS\n",
    "df_filtered = df_new[df_new['INVOICELINEUSERSTATUS'].isin(['Paid', 'NP - Not to be Paid'])]\n",
    "\n",
    "# Step 4: Check for positive day differences\n",
    "def check_day_differences(row):\n",
    "    if row['Days_Treatment'] > 0 or row['Days_Received'] > 0 or row['Days_Discharge'] > 0 or row['Days_Invoice'] > 0:\n",
    "        return \"decline\"\n",
    "    return \"proceed\"\n",
    "\n",
    "df_filtered['Status'] = df_filtered.apply(check_day_differences, axis=1)\n",
    "\n",
    "# Step 5: Filter rows that are marked as \"proceed\"\n",
    "df_filtered = df_filtered[df_filtered['Status'] == \"proceed\"]\n",
    "\n",
    "# Step 6: Check POLICYSTATUS\n",
    "df_filtered['Status'] = df_filtered['POLICYSTATUS'].apply(lambda x: \"proceed\" if x == \"Live\" else \"decline\")\n",
    "\n",
    "# Step 7: Filter rows that are marked as \"proceed\"\n",
    "df_filtered = df_filtered[df_filtered['Status'] == \"proceed\"]\n",
    "\n",
    "# Step 8: Check AUTHENTICATION_TYPE\n",
    "def check_authentication(auth_type):\n",
    "    if auth_type in [\"Blank\", \"UNAUTHORISED\", \"Off Smart\"]:\n",
    "        return \"decline\"\n",
    "    return \"proceed\"\n",
    "\n",
    "df_filtered['Status'] = df_filtered['AUTHENTICATION_TYPE'].apply(check_authentication)\n",
    "\n",
    "# Step 9: Filter rows that are marked as \"proceed\"\n",
    "df_filtered = df_filtered[df_filtered['Status'] == \"proceed\"]\n",
    "\n",
    "# Step 10: Use LLaMA for diagnosis and prescription verification\n",
    "def llama_diagnosis_verification(diagnosis, prescription):\n",
    "    # Prepare the prompt to pass to LLaMA model\n",
    "    prompt = f\"Check if the prescription '{prescription}' matches the diagnosis '{diagnosis}' based on historical medical data standards.\"\n",
    "    \n",
    "    # Call to LLaMA's `chat` function\n",
    "    response_stream = ollama.chat(\n",
    "        model='llama3.2-vision',\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Capture response stream\n",
    "    llama_response = \"\"\n",
    "    for token in response_stream:\n",
    "        llama_response += token['message']['content']\n",
    "    \n",
    "    # Process the response from LLaMA (Example: interpreting \"proceed\" or \"decline\")\n",
    "    if \"match\" in llama_response.lower():\n",
    "        return \"proceed\"\n",
    "    else:\n",
    "        return \"decline\"\n",
    "\n",
    "# Apply the LLaMA check to each row\n",
    "df_new['Status'] = df_new.apply(lambda row: llama_diagnosis_verification(row['FIRSTDIAGNOSIS'], row['Prescription']), axis=1)\n",
    "\n",
    "# Filter rows that are marked as \"proceed\" by LLaMA\n",
    "df_final = df_new[df_new['Status'] == \"proceed\"]\n",
    "\n",
    "# Step 12: Check if Prescribed Amount matches INVOICEDAMOUNT\n",
    "df_final['Status'] = df_final.apply(lambda row: \"decline\" if row['Prescribed Amount'] != row['INVOICEDAMOUNT'] else \"proceed\", axis=1)\n",
    "\n",
    "# Final filter for rows marked as \"proceed\"\n",
    "df_final = df_final[df_final['Status'] == \"proceed\"]\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60978fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
